import logging
import os

import pandas as pd
from PySide6.QtCore import QObject, Signal, Slot
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.logger import configure
from stable_baselines3.common.monitor import Monitor

from modules.env import TrainingEnv, TradingEnv, ActionMaskWrapper


class PPOAgentSB3:
    def __init__(self):
        super().__init__()
        # ãƒ©ãƒƒãƒ—ã—ãªã„ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ç’°å¢ƒä¿æŒç”¨
        self.env_raw = None
        # çµæœä¿æŒç”¨è¾æ›¸
        self.results = dict()
        # è¨­å®šå€¤
        self.total_timesteps = 100_000

    def get_env_with_df(self, df: pd.DataFrame) -> Monitor:
        # ç’°å¢ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ
        self.env_raw = env_raw = TrainingEnv(df)
        # ActionMaskWrapper ãƒ©ãƒƒãƒ‘ãƒ¼ã®é©ç”¨
        env = ActionMaskWrapper(env_raw)
        # SB3ã®ç’°å¢ƒãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        check_env(env, warn=True)
        # Monitor ãƒ©ãƒƒãƒ‘ãƒ¼ã®é©ç”¨
        env_monitor = Monitor(env)

        return env_monitor

    def train(self, df: pd.DataFrame, path_model: str, log_dir: str, new_model: bool = False):
        custom_logger = configure(log_dir, ["stdout", "csv", "tensorboard"])  # å‡ºåŠ›å½¢å¼ã‚’æŒ‡å®š

        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        env = self.get_env_with_df(df)
        # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        if not new_model and os.path.exists(path_model):
            print(f"ãƒ¢ãƒ‡ãƒ« {path_model} ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
            try:
                model = PPO.load(path_model, env, verbose=1)
            except ValueError:
                print("èª­ã¿è¾¼ã¿æ™‚ã€ä¾‹å¤– ValueError ãŒç™ºç”Ÿã—ãŸã®ã§æ–°è¦ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
                model = PPO("MlpPolicy", env, verbose=1)
        else:
            print(f"æ–°è¦ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚")
            model = PPO("MlpPolicy", env, verbose=1)

        # ãƒ­ã‚¬ãƒ¼ã‚’å·®ã—æ›¿ãˆ
        model.set_logger(custom_logger)

        # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
        model.learn(total_timesteps=self.total_timesteps)

        # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {path_model} ã«ä¿å­˜ã—ã¾ã™ã€‚")
        model.save(path_model)

        # å­¦ç¿’ç’°å¢ƒã®è§£æ”¾
        env.close()

    def infer(self, df: pd.DataFrame, path_model: str) -> bool:
        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        env = self.get_env_with_df(df)

        # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        if os.path.exists(path_model):
            print(f"ãƒ¢ãƒ‡ãƒ« {path_model} ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")
        else:
            print(f"ãƒ¢ãƒ‡ãƒ«ã‚’ {path_model} ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return False
        try:
            model = PPO.load(path_model, env, verbose=1)
        except ValueError as e:
            print(e)
            return False

        self.results["obs"] = list()
        self.results["reward"] = list()
        obs, _ = env.reset()
        done = False
        while not done:
            action, _states = model.predict(obs)
            obs, reward, done, truncated, info = env.step(action)
            # è¦³æ¸¬å€¤ãƒˆãƒ¬ãƒ³ãƒ‰æˆç”¨
            self.results["obs"].append(obs)
            # å ±é…¬åˆ†å¸ƒä½œæˆç”¨
            self.results["reward"].append(reward)

        # å–å¼•å†…å®¹
        self.results["transaction"] = self.env_raw.getTransaction()

        # å­¦ç¿’ç’°å¢ƒã®è§£æ”¾
        env.close()

        return True


class AgentWorker(QObject):
    # å£²è²·ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€šçŸ¥
    notifyAction = Signal(int)
    finished = Signal()

    def __init__(self, path_model: str, autopilot: bool):
        super().__init__()
        self.logger = logging.getLogger(__name__)
        self.autopilot = autopilot
        self._running = True
        self._stop_flag = False

        # å­¦ç¿’ç’°å¢ƒã®å–å¾—
        self.env = env = ActionMaskWrapper(TradingEnv())
        env.reset()
        # å­¦ç¿’æ¸ˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.model = PPO.load(path_model, env)

    @Slot(float, float, float)
    def addData(self, ts, price, volume):
        obs = self.env.receive_tick(ts, price, volume)  # çŠ¶æ…‹æ›´æ–°ã®ã¿
        action, _ = self.model.predict(obs)  # ãƒã‚¹ã‚¯ã¯å†…éƒ¨ã§åæ˜ 
        if self.autopilot:
            # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
            # ğŸ§¿ å£²è²·ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€šçŸ¥ã™ã‚‹ã‚·ã‚°ãƒŠãƒ«
            self.notifyAction.emit(action)
            # +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
        obs, reward, _, _, info = self.env.step(action)  # ãƒã‚¹ã‚¯æ›´æ–°ã¨å ±é…¬è¨ˆç®—

    @Slot(bool)
    def setAutoPilotStatus(self, state: bool):
        self.autopilot = state
        self.logger.info(f"{__name__}: autopilot is set to {state}.")

    @Slot()
    def stop(self):
        """çµ‚äº†å‡¦ç†"""
        self._stop_flag = True
        self.finished.emit()


